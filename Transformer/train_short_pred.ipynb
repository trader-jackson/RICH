{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaomi\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from exp.exp_pred import Exp_pred\n",
    "from exp.exp_mae import Exp_mae\n",
    "\n",
    "from data.stock_data_handle import Stock_Data\n",
    "import tools as utils\n",
    "import time\n",
    "\n",
    "import pdb\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "fix_seed = 2022\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='[Transformer] Long Sequences Forecasting')\n",
    "\n",
    "parser.add_argument('--model', type=str, default='Transformer',help='model of the experiment')\n",
    "parser.add_argument('--project_name', type=str, default='baseline',help='name of the experiment')\n",
    "\n",
    "parser.add_argument('--data_name', type=str, default='CSI', help='')\n",
    "parser.add_argument('--data_type', type=str, default='stock', help='stock')\n",
    "parser.add_argument('--root_path', type=str, default='data/', help='root path of the data file')\n",
    "parser.add_argument('--full_stock_path', type=str, default='processed_data/CSI/', help='root path of the data file')\n",
    "\n",
    "parser.add_argument('--exp_type', type=str, default='pred', help='[mae|pred]')\n",
    "\n",
    "parser.add_argument('--seq_len', type=int, default=60, help='input series length')\n",
    "parser.add_argument('--label_len', type=int, default=1, help='help series length')\n",
    "parser.add_argument('--pred_len', type=int, default=1, help='predict series length')\n",
    "\n",
    "parser.add_argument('--enc_in', type=int, default=96, help='encoder input size: cov+technical indicators')\n",
    "parser.add_argument('--dec_in', type=int, default=96, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=96, help='output size')\n",
    "\n",
    "parser.add_argument('--short_term_len', type=int, default=1, help='short term prediction len')\n",
    "parser.add_argument('--long_term_len', type=int, default=5, help='long term prediction len')\n",
    "parser.add_argument('--pred_type', type=str, default='long_term_len', help='type of prediction')\n",
    "\n",
    "parser.add_argument('--d_model', type=int, default=128, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=4, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=256, help='dimension of fcn')\n",
    "\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "\n",
    "parser.add_argument('--activation', type=str, default='gelu',help='activation')\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "\n",
    "parser.add_argument('--rank_alpha', type=float, default=0.1, help='weight of rank loss') # adjust\n",
    "\n",
    "parser.add_argument('--itr', type=int, default=2, help='each params run iteration')\n",
    "parser.add_argument('--train_epochs', type=int, default=30, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='input data batch size')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--adjust_interval', type=int, default=1, help='lr adjust interval')\n",
    "parser.add_argument('--des', type=str, default='pred',help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse',help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1',help='adjust learning rate')\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.project_name=\"transformer_CSI_predShort\"\n",
    "args.exp_type=\"pred\"\n",
    "args.root_path=\"../../data/\"\n",
    "args.full_stock_path=\"data/CSI\"\n",
    "args.enc_in=10\n",
    "args.dec_in=10\n",
    "args.c_out=1\n",
    "args.pred_type=\"label_short_term\"\n",
    "args.rank_alpha=1\n",
    "args.train_epochs=50\n",
    "args.itr=1\n",
    "args.adjust_interval=10\n",
    "args.devices=0\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "python main.py \\\n",
    "      --project_name transformer_CSI_predShort \\\n",
    "      --exp_type pred \\\n",
    "      --data_name CSI \\\n",
    "      --data_type stock \\\n",
    "      --root_path ../../data/ \\\n",
    "      --full_stock_path CSI/ \\\n",
    "      --seq_len 60 \\\n",
    "      --label_len 1 \\\n",
    "      --pred_len 1 \\\n",
    "      --enc_in 10 \\\n",
    "      --dec_in 10 \\\n",
    "      --c_out 1 \\\n",
    "      --short_term_len 1 \\\n",
    "      --long_term_len 5 \\\n",
    "      --pred_type label_short_term \\\n",
    "      --d_model 128 \\\n",
    "      --n_heads 4 \\\n",
    "      --e_layers 2 \\\n",
    "      --d_layers 1 \\\n",
    "      --d_ff 256 \\\n",
    "      --dropout 0.05 \\\n",
    "      --rank_alpha 1.0 \\\n",
    "      --train_epochs 50 \\\n",
    "      --itr 1 \\\n",
    "      --batch_size 32 \\\n",
    "      --learning_rate 0.0001 \\\n",
    "      --adjust_interval 10 \\\n",
    "      --num_workers 10 \\\n",
    "      --devices 0 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores available: 20\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"CPU cores available:\", multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_workers=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict = {'pred': Exp_pred, 'mae': Exp_mae}\n",
    "data_type_dict = {'stock': Stock_Data}\n",
    "Exp = exp_dict[args.exp_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaomi\\Desktop\\Quant\\RL4PO\\StockFormer\\code\\Transformer\\data\\stock_data_handle.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat((df, temp_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate technical indicator...\n",
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaomi\\Desktop\\Quant\\RL4PO\\StockFormer\\code\\Transformer\\preprocess.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate convariate matrix...\n",
      "data shape:  (2743, 88, 106)\n",
      "label shape:  (2, 2743, 88)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data =  data_type_dict[args.data_type](\n",
    "        root_path=args.root_path,\n",
    "        dataset_name=args.data_name,\n",
    "        full_stock_path=args.full_stock_path,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        prediction_len=[args.short_term_len, args.long_term_len]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaomi\\AppData\\Local\\Temp\\ipykernel_37676\\1442900287.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat((df, temp_df))\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "import pandas as pd\n",
    "ticker_list = config.use_ticker_dict[args.data_name]\n",
    "border_dates = config.date_dict[args.data_name]\n",
    "stock_num = len(ticker_list)\n",
    "\n",
    "full_stock_dir =  '../../data/CSI/'\n",
    "\n",
    "df = pd.DataFrame([], columns=['date','open','close','high','low','volume','dopen','dclose','dhigh','dlow','dvolume', 'price', 'tic'])\n",
    "for ticket in ticker_list:\n",
    "    temp_df = pd.read_csv(os.path.join(full_stock_dir,ticket+'.csv'), usecols=['date', 'open', 'close', 'high', 'low', 'volume', 'dopen', 'dclose', 'dhigh', 'dlow', 'dvolume', 'price'])\n",
    "\n",
    "    temp_df['date'] = temp_df['date'].apply(lambda x:str(x))\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
    "    temp_df['label_short_term'] = temp_df['close'].pct_change(periods=args.short_term_len).shift(periods=(args.short_term_len))\n",
    "    temp_df['label_long_term'] = temp_df['close'].pct_change(periods=args.long_term_len).shift(periods=(args.long_term_len))\n",
    "    temp_df['tic'] = ticket\n",
    "    df = pd.concat((df, temp_df))\n",
    "df = df.sort_values(by=['date','tic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from preprocess import FeatureEngineer\n",
    "fe = FeatureEngineer(\n",
    "            use_technical_indicator=True,\n",
    "            tech_indicator_list = config.TECHNICAL_INDICATORS_LIST,\n",
    "            use_turbulence=False,\n",
    "            user_defined_feature = False)\n",
    "\n",
    "print(\"generate technical indicator...\")\n",
    "df = fe.preprocess_data(df)\n",
    "\n",
    "import datetime\n",
    "# add covariance matrix as states\n",
    "df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "df.index = df.date.factorize()[0]\n",
    "\n",
    "cov_list = []\n",
    "return_list = []\n",
    "\n",
    "# look back is one year\n",
    "print(\"generate convariate matrix...\")\n",
    "lookback=252\n",
    "for i in range(lookback,len(df.index.unique())):\n",
    "    data_lookback = df.loc[i-lookback:i,:]\n",
    "    price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close') \n",
    "    return_lookback = price_lookback.pct_change().dropna()\n",
    "    return_list.append(return_lookback)\n",
    "\n",
    "    covs = return_lookback.cov().values \n",
    "    cov_list.append(covs)\n",
    "\n",
    "df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
    "df = df.merge(df_cov, on='date')\n",
    "df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
    "\n",
    "df['date_str'] = df['date'].apply(lambda x: datetime.datetime.strftime(x,'%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dates = df['date_str'].unique().tolist()\n",
    "boarder1_ = dates.index(border_dates[0])\n",
    "boarder1 = dates.index(border_dates[1]) \n",
    "\n",
    "boarder2_ = dates.index(border_dates[2])\n",
    "boarder2 = dates.index(border_dates[3]) \n",
    "\n",
    "boarder3_ = dates.index(border_dates[4])\n",
    "boarder3 = dates.index(border_dates[5]) \n",
    "\n",
    "boarder_end = [boarder1, boarder2, boarder3]\n",
    "boarder_start = [boarder1_, boarder2_, boarder3_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_cov = cov_list.reshape(-1, stock_num, cov_list.shape[1], cov_list.shape[2]) # [day, num_stocks, num_stocks, num_stocks]\n",
    "data_technical = data.reshape(-1, stock_num, len(attr)) # [day, stock_num, technical_len]\n",
    "data_feature = feature_list.reshape(-1, stock_num, len(temporal_feature)) # [day, stock_num, temporal_feature_len=10]\n",
    "data_close = close_list.reshape(-1, stock_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2743, 88, 88, 88), (2743, 88, 8), (2743, 88, 10), (2743, 88))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cov.shape,data_technical.shape,data_feature.shape,data_close.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_short_term = np.array(df['label_short_term'].values.tolist()).reshape(-1, stock_num)\n",
    "label_long_term = np.array(df['label_long_term'].values.tolist()).reshape(-1, stock_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2743, 88, 106), (2, 2743, 88))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_all = np.concatenate((data_cov[:, 0, :, :], data_technical, data_feature), axis=-1) # [days, num_stocks, cov+technical_len+feature_len]\n",
    "label_all = np.stack((label_short_term, label_long_term), axis=0) # [2, days, num_stocks, 1]\n",
    "data_all.shape,label_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (2743, 88, 106)\n",
      "label shape:  (2, 2743, 88)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dates = np.array(dates)\n",
    "data_close = data_close\n",
    "\n",
    "print(\"data shape: \",data_all.shape)\n",
    "print(\"label shape: \",label_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddw1metg\n",
      "pred_transformer_CSI_predShort_CSI_alpha1_sl60_pl1_enc10_cout1_dm128_nh4_el2_dl1_df256_pred_0_dtCSI_idddw1metg\n"
     ]
    }
   ],
   "source": [
    "id = utils.generate_id()\n",
    "ii=0\n",
    "setting = '{}_{}_{}_alpha{}_sl{}_pl{}_enc{}_cout{}_dm{}_nh{}_el{}_dl{}_df{}_{}_{}_dt{}_id{}'.format(args.exp_type, args.project_name, args.data_name, str(args.rank_alpha).replace('.','_'),\n",
    "            args.seq_len, args.pred_len, args.enc_in, args.c_out,\n",
    "            args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.des, ii, args.data_name, id)\n",
    "print(id)\n",
    "print(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stock'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import config\n",
    "class DatasetStock_PRED(Dataset):\n",
    "    def __init__(self, stock: Stock_Data, type='train', feature=config.TEMPORAL_FEATURE, pred_type='label_short_term'):\n",
    "        super().__init__()\n",
    "        assert type in ['train', 'test', 'valid']\n",
    "        assert pred_type in ['label_short_term', 'label_long_term']\n",
    "        \n",
    "        pos = stock.type_map[type]\n",
    "\n",
    "        self.label_type = stock.pred_type_map[pred_type]\n",
    "        self.start_pos = stock.boarder_start[pos]\n",
    "        self.end_pos = stock.boarder_end[pos]+1\n",
    "       \n",
    "\n",
    "        self.feature_len = len(feature)\n",
    "        self.feature_day_len = stock.seq_len\n",
    "        self.data = stock.data_all\n",
    "        self.label = stock.label_all\n",
    "        \n",
    "        self.dates = stock.dates[self.start_pos: self.end_pos]\n",
    "        self.data_close = stock.data_close[self.start_pos: self.end_pos]\n",
    "\n",
    "        # pdb.set_trace()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        position = self.start_pos+index\n",
    "        seq_x = self.data[position-self.feature_day_len+1:position+1, :, -self.feature_len:].transpose(1,0,2) #[days, num_stocks, feature]-> [num_stocks, days, feature]\n",
    "        seq_x_dec = seq_x[:, -1:, :]\n",
    "\n",
    "        seq_y = self.label[self.label_type, index, :]\n",
    "        return seq_x, seq_x_dec, seq_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.end_pos-self.start_pos#len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log\\pred_transformer_CSI_predShort_1_c2xn1b54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "log_dir = os.path.join('log', 'pred_'+args.project_name+'_'+str(args.rank_alpha)+'_'+id)\n",
    "print(log_dir)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "data_all = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.embed import DataEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_short_term\n",
      "59 1936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.DatasetStock_PRED at 0x17e1210c850>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x17e1210ce10>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_dict = {\n",
    "    'stock': DatasetStock_PRED,\n",
    "}\n",
    "\n",
    "\n",
    "flag=\"train\"\n",
    "if flag == 'train':\n",
    "    shuffle_flag = True; drop_last = False; batch_size = args.batch_size\n",
    "else:\n",
    "    shuffle_flag = False; drop_last = True; batch_size = args.batch_size\n",
    "\n",
    "dataset = dataset_dict[args.data_type](data, type=flag, pred_type=args.pred_type)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=0,\n",
    "    drop_last=drop_last)\n",
    "\n",
    "dataset, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88, 60, 10), (88, 1, 10), (88,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c=dataset[0]\n",
    "a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embedding=DataEmbedding(c_in=10,d_model=512,dropout=args.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 60, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_x = total_embedding(torch.tensor(a).float())  # Shape will be [1, 100, 512]\n",
    "pe_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 88, 60, 10])\n",
      "Labels batch shape: torch.Size([32, 88])\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, trainx2,train_labels = next(iter(data_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 60, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.stock_data_handle import Stock_Data,DatasetStock,DatasetStock_PRED\n",
    "from exp.exp_basic import Exp_Basic\n",
    "from models.transformer import Transformer_base as Transformer\n",
    "from tools import EarlyStopping, adjust_learning_rate\n",
    "from metrics import metric, ranking_loss\n",
    "import tools as utils\n",
    "import metrics_object as metrics_object\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "dataset_dict = {\n",
    "    'stock': DatasetStock_PRED,\n",
    "}\n",
    "model_dict = {\n",
    "            'Transformer':Transformer,\n",
    "        }\n",
    "\n",
    "if args.model=='Transformer':\n",
    "    model = model_dict[args.model](\n",
    "        # self.args\n",
    "        args.enc_in,\n",
    "        args.dec_in, \n",
    "        args.c_out,\n",
    "        args.d_model, \n",
    "        args.n_heads, \n",
    "        args.e_layers,\n",
    "        args.d_layers, \n",
    "        args.d_ff,\n",
    "        args.dropout, \n",
    "        args.activation\n",
    "        # self.device\n",
    "    )\n",
    "\n",
    "if args.use_multi_gpu and args.use_gpu:\n",
    "    model = nn.DataParallel(model, device_ids=args.device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Exp_pred(Exp_Basic):\n",
    "    def __init__(self, args, data_all, id):\n",
    "        super(Exp_pred, self).__init__(args)\n",
    "        log_dir = os.path.join('log', 'pred_'+args.project_name+'_'+str(args.rank_alpha)+'_'+id)\n",
    "        \n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "        self.data_all = data_all\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_dict = {\n",
    "            'Transformer':Transformer,\n",
    "        }\n",
    "\n",
    "        if self.args.model=='Transformer':\n",
    "            model = model_dict[self.args.model](\n",
    "                # self.args\n",
    "                self.args.enc_in,\n",
    "                self.args.dec_in, \n",
    "                self.args.c_out,\n",
    "                self.args.d_model, \n",
    "                self.args.n_heads, \n",
    "                self.args.e_layers,\n",
    "                self.args.d_layers, \n",
    "                self.args.d_ff,\n",
    "                self.args.dropout, \n",
    "                self.args.activation\n",
    "                # self.device\n",
    "            )\n",
    "\n",
    "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
    "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
    "        \n",
    "        return model.float()\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        args = self.args\n",
    "\n",
    "        if flag == 'train':\n",
    "            shuffle_flag = True; drop_last = False; batch_size = args.batch_size\n",
    "        else:\n",
    "            shuffle_flag = False; drop_last = True; batch_size = args.batch_size\n",
    "      \n",
    "        dataset = dataset_dict[self.args.data_type](self.data_all, type=flag, pred_type=self.args.pred_type)\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "\n",
    "        return dataset, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "    \n",
    "    def _select_criterion(self):\n",
    "        criterion =  nn.MSELoss()\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion, metric_builders, stage='test'):\n",
    "        self.model.eval()\n",
    "        total_loss = []\n",
    "        metric_objs = [builder(stage) for builder in metric_builders]\n",
    "\n",
    "        for i, (batch_x1, batch_x2, batch_y) in enumerate(vali_loader):\n",
    "            bs, stock_num = batch_x1.shape[0], batch_x1.shape[1]\n",
    "            batch_x1 = batch_x1.reshape(-1, batch_x1.shape[-2], batch_x1.shape[-1]).float().to(self.device)\n",
    "            batch_x2 = batch_x2.reshape(-1, batch_x2.shape[-2], batch_x2.shape[-1]).float().to(self.device)\n",
    "            batch_y = batch_y.float().to(self.device)\n",
    "            \n",
    "            _, _, output = self.model(batch_x1, batch_x2)\n",
    "\n",
    "            output = output.reshape(bs,stock_num)\n",
    "            loss = criterion(output, batch_y) + self.args.rank_alpha * ranking_loss(output, batch_y)\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for metric in metric_objs:\n",
    "                    metric.update(output, batch_y)\n",
    "\n",
    "        total_loss = np.average(total_loss)\n",
    "        self.model.train()\n",
    "        return total_loss, metric_objs\n",
    "        \n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag = 'train')\n",
    "        vali_data, vali_loader = self._get_data(flag = 'valid')\n",
    "        test_data, test_loader = self._get_data(flag = 'test')\n",
    "\n",
    "        metrics_builders = [\n",
    "        metrics_object.MIRRTop1,\n",
    "    ]\n",
    "\n",
    "        path = os.path.join('./checkpoints/',setting)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "        \n",
    "        train_steps = len(train_loader)        \n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion =  self._select_criterion()\n",
    "\n",
    "        metric_objs = [builder('train') for builder in metrics_builders]\n",
    "\n",
    "        valid_loss_global = np.inf\n",
    "        best_model_index = -1\n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "            \n",
    "            self.model.train()\n",
    "            for i, (batch_x1, batch_x2, batch_y) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                # pdb.set_trace()\n",
    "                bs, stock_num = batch_x1.shape[0], batch_x1.shape[1]\n",
    "                batch_x1 = batch_x1.reshape(-1, batch_x1.shape[-2], batch_x1.shape[-1]).float().to(self.device)\n",
    "                batch_x2 = batch_x2.reshape(-1, batch_x2.shape[-2], batch_x2.shape[-1]).float().to(self.device)\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "                _,_, output = self.model(batch_x1, batch_x2)\n",
    "            \n",
    "                output = output.reshape(bs,stock_num)\n",
    "        \n",
    "                loss = criterion(output, batch_y) + self.args.rank_alpha * ranking_loss(output, batch_y)\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                model_optim.zero_grad()\n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "                \n",
    "                if (i+1) % 100==0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time()-time_now)/iter_count\n",
    "                    left_time = speed*((self.args.train_epochs - epoch)*train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for metric in metric_objs:\n",
    "                        metric.update(output, batch_y)\n",
    "\n",
    "\n",
    "            train_loss = np.average(train_loss)\n",
    "            valid_loss, valid_metrics = self.vali(vali_data, vali_loader, criterion, metrics_builders, stage='valid')\n",
    "            test_loss, test_metrics = self.vali(test_data, test_loader, criterion, metrics_builders, stage='test')\n",
    "\n",
    "            self.writer.add_scalar('Train/loss', train_loss, epoch)\n",
    "            self.writer.add_scalar('Valid/loss', valid_loss, epoch)\n",
    "            self.writer.add_scalar('Test/loss', test_loss, epoch)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            all_logs = {\n",
    "                metric.name: metric.value for metric in metric_objs + valid_metrics + test_metrics\n",
    "            }\n",
    "            for name, value in all_logs.items():\n",
    "                self.writer.add_scalar(name, value.mean(), global_step=epoch)\n",
    "\n",
    "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Valid Loss: {3:.7f} Test Loss: {3:.7f}\".format(\n",
    "                epoch + 1, train_steps, train_loss, valid_loss, test_loss))\n",
    "            \n",
    "            torch.save(self.model.state_dict(), path+'/'+'checkpoint_{0}.pth'.format(epoch+1))\n",
    "\n",
    "            if valid_loss.item() < valid_loss_global:\n",
    "                best_model_index = epoch+1\n",
    "\n",
    "            adjust_learning_rate(model_optim, epoch+1, self.args)\n",
    "            \n",
    "        best_model_path = path+'/'+'checkpoint_{0}.pth'.format(best_model_index)\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "        print('best model index: ', best_model_index)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def test(self, setting):\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "\n",
    "        outputs = []\n",
    "        real = []\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        metrics_builders = [\n",
    "        metrics_object.MIRRTop1,\n",
    "        metrics_object.RankIC\n",
    "    ]\n",
    "        \n",
    "        metric_objs = [builder('test') for builder in metrics_builders]\n",
    "        \n",
    "        for i, (batch_x1, batch_x2, batch_y) in enumerate(test_loader):\n",
    "            bs, stock_num = batch_x1.shape[0], batch_x2.shape[1]\n",
    "            batch_x1 = batch_x1.reshape(-1, batch_x1.shape[-2], batch_x1.shape[-1]).float().to(self.device)\n",
    "            batch_x2 = batch_x2.reshape(-1, batch_x2.shape[-2], batch_x2.shape[-1]).float().to(self.device)\n",
    "            batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "            _,_, output = self.model(batch_x1, batch_x2)\n",
    "\n",
    "            output = output.reshape(bs,stock_num)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for metric in metric_objs:\n",
    "                    metric.update(output, batch_y)\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting +'/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        all_logs = {\n",
    "                metric.name: metric.value for metric in metric_objs\n",
    "            }\n",
    "        for name, value in all_logs.items():\n",
    "            print(name, value.mean())\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "log\\pred_transformer_CSI_predShort_1_ddw1metg\n"
     ]
    }
   ],
   "source": [
    "exp = Exp(args, data, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_short_term\n",
      "59 1936\n",
      "label_short_term\n",
      "1693 2421\n",
      "label_short_term\n",
      "1995 2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m model_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Optionally update the progress bar with the current loss and estimated time remaining\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124miters: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, epoch: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m | loss: \u001b[39m\u001b[38;5;132;01m{2:.7f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem()))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import the tqdm progress bar\n",
    "\n",
    "train_data, train_loader = exp._get_data(flag='train')\n",
    "vali_data, vali_loader = exp._get_data(flag='valid')\n",
    "test_data, test_loader = exp._get_data(flag='test')\n",
    "\n",
    "metrics_builders = [\n",
    "    metrics_object.MIRRTop1,\n",
    "]\n",
    "\n",
    "path = os.path.join('./checkpoints/', setting)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "time_now = time.time()\n",
    "\n",
    "train_steps = len(train_loader)\n",
    "model_optim = exp._select_optimizer()\n",
    "criterion = exp._select_criterion()\n",
    "\n",
    "metric_objs = [builder('train') for builder in metrics_builders]\n",
    "\n",
    "valid_loss_global = np.inf\n",
    "best_model_index = -1\n",
    "\n",
    "for epoch in range(exp.args.train_epochs):\n",
    "    iter_count = 0\n",
    "    train_loss = []\n",
    "    \n",
    "    exp.model.train()\n",
    "    # Wrap the training loader with tqdm for a progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{exp.args.train_epochs}\", leave=False)\n",
    "    for i, (batch_x1, batch_x2, batch_y) in enumerate(pbar):\n",
    "        iter_count += 1\n",
    "        bs, stock_num = batch_x1.shape[0], batch_x1.shape[1]\n",
    "        batch_x1 = batch_x1.reshape(-1, batch_x1.shape[-2], batch_x1.shape[-1]).float().to(exp.device)\n",
    "        batch_x2 = batch_x2.reshape(-1, batch_x2.shape[-2], batch_x2.shape[-1]).float().to(exp.device)\n",
    "        batch_y = batch_y.float().to(exp.device)\n",
    "\n",
    "        _, _, output = exp.model(batch_x1, batch_x2)\n",
    "        output = output.reshape(bs, stock_num)\n",
    "\n",
    "        loss = criterion(output, batch_y) + exp.args.rank_alpha * ranking_loss(output, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "        \n",
    "        # Optionally update the progress bar with the current loss and estimated time remaining\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.7f}\"})\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "            speed = (time.time()-time_now)/iter_count\n",
    "            left_time = speed*((exp.args.train_epochs - epoch)*train_steps - i)\n",
    "            print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "            iter_count = 0\n",
    "            time_now = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for metric in metric_objs:\n",
    "                metric.update(output, batch_y)\n",
    "\n",
    "    train_loss = np.average(train_loss)\n",
    "    valid_loss, valid_metrics = exp.vali(vali_data, vali_loader, criterion, metrics_builders, stage='valid')\n",
    "    test_loss, test_metrics = exp.vali(test_data, test_loader, criterion, metrics_builders, stage='test')\n",
    "\n",
    "    exp.writer.add_scalar('Train/loss', train_loss, epoch)\n",
    "    exp.writer.add_scalar('Valid/loss', valid_loss, epoch)\n",
    "    exp.writer.add_scalar('Test/loss', test_loss, epoch)\n",
    "\n",
    "    all_logs = {\n",
    "        metric.name: metric.value for metric in metric_objs + valid_metrics + test_metrics\n",
    "    }\n",
    "    for name, value in all_logs.items():\n",
    "        exp.writer.add_scalar(name, value.mean(), global_step=epoch)\n",
    "\n",
    "    print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Valid Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "        epoch + 1, train_steps, train_loss, valid_loss, test_loss))\n",
    "    \n",
    "    torch.save(exp.model.state_dict(), os.path.join(path, f'checkpoint_{epoch+1}.pth'))\n",
    "\n",
    "    if valid_loss.item() < valid_loss_global:\n",
    "        best_model_index = epoch + 1\n",
    "        valid_loss_global = valid_loss.item()  # Update the best validation loss\n",
    "\n",
    "    adjust_learning_rate(model_optim, epoch+1, exp.args)\n",
    "    \n",
    "best_model_path = os.path.join(path, f'checkpoint_{best_model_index}.pth')\n",
    "exp.model.load_state_dict(torch.load(best_model_path))\n",
    "print('Best model index:', best_model_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08326154202222824]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : pred_transformer_CSI_predShort_CSI_alpha1_sl60_pl1_enc10_cout1_dm128_nh4_el2_dl1_df256_pred_0_dtCSI_idddw1metg>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Task id:  ddw1metg\n",
      "label_short_term\n",
      "59 1936\n",
      "label_short_term\n",
      "1693 2421\n",
      "label_short_term\n",
      "1995 2664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask id: \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Time:\u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[1;32mc:\\Users\\gaomi\\Desktop\\Quant\\RL4PO\\StockFormer\\code\\Transformer\\exp\\exp_pred.py:173\u001b[0m, in \u001b[0;36mExp_pred.train\u001b[1;34m(self, setting)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metric_objs:\n\u001b[1;32m--> 173\u001b[0m             \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(train_loss)\n\u001b[0;32m    177\u001b[0m valid_loss, valid_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvali(vali_data, vali_loader, criterion, metrics_builders, stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gaomi\\Desktop\\Quant\\RL4PO\\StockFormer\\code\\Transformer\\metrics_object.py:48\u001b[0m, in \u001b[0;36mMetricMeter.update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\gaomi\\Desktop\\Quant\\RL4PO\\StockFormer\\code\\Transformer\\metrics.py:86\u001b[0m, in \u001b[0;36mmirr_top1\u001b[1;34m(prediction, gt)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmirr_top1\u001b[39m(prediction: Tensor, gt: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m     85\u001b[0m     pred_top1_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(prediction, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m     mirr \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_top1_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mirr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "print('Task id: ',id)\n",
    "start = time.time()\n",
    "exp.train(setting)\n",
    "end = time.time()\n",
    "print(\"Training Time:\",end-start)\n",
    "\n",
    "print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "exp.test(setting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
